{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1I4i3PnkeKA-ZIO_XhYMO8sEOza-vqT3-","authorship_tag":"ABX9TyMHXwsRHx3YgrdB2MuFsEMi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SRGAN\n","Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, CVPR 2017\n","\n","首次将生成对抗网络用于超分辨率问题。\n","\n","https://github.com/leftthomas/SRGAN/tree/master"],"metadata":{"id":"m63GeF-_l7h0"}},{"cell_type":"markdown","source":["## 准备数据集"],"metadata":{"id":"Xvbj4zPrmBwZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKdzCl8ClnWv"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["# 引入依赖包"],"metadata":{"id":"5NOB-FmamEIY"}},{"cell_type":"code","source":["!pip3 install tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcIzeGTxuDok","executionInfo":{"status":"ok","timestamp":1712412198933,"user_tz":-480,"elapsed":7139,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}},"outputId":"98050a36-9414-45f8-a690-4377feef359c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"]}]},{"cell_type":"code","source":["import math\n","import torch\n","import torch.utils.data\n","import torch.optim as optim\n","from torch import nn\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","import torchvision.utils as utils\n","from torchvision.models.vgg import vgg16\n","from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n","from torchvision.utils import make_grid\n","import os\n","from os import listdir\n","from os.path import join\n","from math import log10\n","from PIL import Image\n","from tqdm import tqdm"],"metadata":{"id":"i71aqz6JmGlc","executionInfo":{"status":"ok","timestamp":1712413404481,"user_tz":-480,"elapsed":427,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["## 定义图像转换管道与数据集相关类"],"metadata":{"id":"VUDheT9opuH2"}},{"cell_type":"code","source":["def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n","\n","\n","def calculate_valid_crop_size(crop_size, upscale_factor):\n","    return crop_size - (crop_size % upscale_factor)\n","\n","\n","def train_hr_transform(crop_size):\n","    return Compose([\n","        RandomCrop(crop_size),\n","        ToTensor(),\n","    ])\n","\n","\n","def train_lr_transform(crop_size, upscale_factor):\n","    return Compose([\n","        ToPILImage(),\n","        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n","        ToTensor()\n","    ])\n","\n","\n","def display_transform():\n","    return Compose([\n","        ToPILImage(),\n","        Resize(400),\n","        CenterCrop(400),\n","        ToTensor()\n","    ])\n","\n","\n","class TrainDatasetFromFolder(Dataset):\n","    def __init__(self, dataset_dir, crop_size, upscale_factor):\n","        super(TrainDatasetFromFolder, self).__init__()\n","        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n","        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n","        self.hr_transform = train_hr_transform(crop_size)\n","        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n","\n","    def __getitem__(self, index):\n","        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n","        lr_image = self.lr_transform(hr_image)\n","        return lr_image, hr_image\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","\n","class ValDatasetFromFolder(Dataset):\n","    def __init__(self, dataset_dir, upscale_factor):\n","        super(ValDatasetFromFolder, self).__init__()\n","        self.upscale_factor = upscale_factor\n","        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n","\n","    def __getitem__(self, index):\n","        hr_image = Image.open(self.image_filenames[index])\n","        w, h = hr_image.size\n","        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n","        lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n","        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n","        hr_image = CenterCrop(crop_size)(hr_image)\n","        lr_image = lr_scale(hr_image)\n","        hr_restore_img = hr_scale(lr_image)\n","        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","\n","class TestDatasetFromFolder(Dataset):\n","    def __init__(self, dataset_dir, upscale_factor):\n","        super(TestDatasetFromFolder, self).__init__()\n","        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'\n","        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'\n","        self.upscale_factor = upscale_factor\n","        self.lr_filenames = [join(self.lr_path, x) for x in listdir(self.lr_path) if is_image_file(x)]\n","        self.hr_filenames = [join(self.hr_path, x) for x in listdir(self.hr_path) if is_image_file(x)]\n","\n","    def __getitem__(self, index):\n","        image_name = self.lr_filenames[index].split('/')[-1]\n","        lr_image = Image.open(self.lr_filenames[index])\n","        w, h = lr_image.size\n","        hr_image = Image.open(self.hr_filenames[index])\n","        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w), interpolation=Image.BICUBIC)\n","        hr_restore_img = hr_scale(lr_image)\n","        return image_name, ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n","\n","    def __len__(self):\n","        return len(self.lr_filenames)"],"metadata":{"id":"CLtbjE_Op4Op","executionInfo":{"status":"ok","timestamp":1712412564905,"user_tz":-480,"elapsed":470,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## 定义网络所用残差块、上采样块"],"metadata":{"id":"cx52F2CGmvNc"}},{"cell_type":"code","source":["class ResidualBlock(nn.Module):\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(channels)\n","        self.prelu = nn.PReLU()\n","        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(channels)\n","\n","    def forward(self, x):\n","        residual = self.conv1(x)\n","        residual = self.bn1(residual)\n","        residual = self.prelu(residual)\n","        residual = self.conv2(residual)\n","        residual = self.bn2(residual)\n","\n","        return x + residual"],"metadata":{"id":"vMvTBFf8md2V","executionInfo":{"status":"ok","timestamp":1712412569037,"user_tz":-480,"elapsed":2,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class UpsampleBLock(nn.Module):\n","    def __init__(self, in_channels, up_scale):\n","        super(UpsampleBLock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n","        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n","        self.prelu = nn.PReLU()\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.pixel_shuffle(x)\n","        x = self.prelu(x)\n","        return x"],"metadata":{"id":"rQIUROwkmj4M","executionInfo":{"status":"ok","timestamp":1712412571056,"user_tz":-480,"elapsed":1,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## 定义GAN网络的生成器、判别器"],"metadata":{"id":"ft_kpZYvmP-b"}},{"cell_type":"code","source":["class Generator(nn.Module):\n","    def __init__(self, scale_factor):\n","        upsample_block_num = int(math.log(scale_factor, 2))\n","\n","        super(Generator, self).__init__()\n","        self.block1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n","            nn.PReLU()\n","        )\n","        self.block2 = ResidualBlock(64)\n","        self.block3 = ResidualBlock(64)\n","        self.block4 = ResidualBlock(64)\n","        self.block5 = ResidualBlock(64)\n","        self.block6 = ResidualBlock(64)\n","        self.block7 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64)\n","        )\n","        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n","        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n","        self.block8 = nn.Sequential(*block8)\n","\n","    def forward(self, x):\n","        block1 = self.block1(x)\n","        block2 = self.block2(block1)\n","        block3 = self.block3(block2)\n","        block4 = self.block4(block3)\n","        block5 = self.block5(block4)\n","        block6 = self.block6(block5)\n","        block7 = self.block7(block6)\n","        block8 = self.block8(block1 + block7)\n","\n","        return (torch.tanh(block8) + 1) / 2"],"metadata":{"id":"UHo-4NQymXWo","executionInfo":{"status":"ok","timestamp":1712412573713,"user_tz":-480,"elapsed":469,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2),\n","\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(512, 1024, kernel_size=1),\n","            nn.LeakyReLU(0.2),\n","            nn.Conv2d(1024, 1, kernel_size=1)\n","        )\n","\n","    def forward(self, x):\n","        batch_size = x.size(0)\n","        return torch.sigmoid(self.net(x).view(batch_size))"],"metadata":{"id":"RmUmzYg9mqvq","executionInfo":{"status":"ok","timestamp":1712412576594,"user_tz":-480,"elapsed":428,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["## 定义损失函数"],"metadata":{"id":"JFpaPRcDoB2e"}},{"cell_type":"code","source":["class TVLoss(nn.Module):\n","    def __init__(self, tv_loss_weight=1):\n","        super(TVLoss, self).__init__()\n","        self.tv_loss_weight = tv_loss_weight\n","\n","    def forward(self, x):\n","        batch_size = x.size()[0]\n","        h_x = x.size()[2]\n","        w_x = x.size()[3]\n","        count_h = self.tensor_size(x[:, :, 1:, :])\n","        count_w = self.tensor_size(x[:, :, :, 1:])\n","        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n","        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n","        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n","\n","    @staticmethod\n","    def tensor_size(t):\n","        return t.size()[1] * t.size()[2] * t.size()[3]\n","\n","class GeneratorLoss(nn.Module):\n","    def __init__(self):\n","        super(GeneratorLoss, self).__init__()\n","        vgg = vgg16(pretrained=True)\n","        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n","        for param in loss_network.parameters():\n","            param.requires_grad = False\n","        self.loss_network = loss_network\n","        self.mse_loss = nn.MSELoss()\n","        self.tv_loss = TVLoss()\n","\n","    def forward(self, out_labels, out_images, target_images):\n","        # Adversarial Loss\n","        adversarial_loss = torch.mean(1 - out_labels)\n","        # Perception Loss\n","        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n","        # Image Loss\n","        image_loss = self.mse_loss(out_images, target_images)\n","        # TV Loss\n","        tv_loss = self.tv_loss(out_images)\n","        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss"],"metadata":{"id":"493USNxloDgL","executionInfo":{"status":"ok","timestamp":1712412579832,"user_tz":-480,"elapsed":566,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## 训练网络的函数"],"metadata":{"id":"HMFwHH6ordqO"}},{"cell_type":"code","source":["CROP_SIZE = 88\n","UPSCALE_FACTOR = 4\n","NUM_EPOCHS = 1\n","\n","train_set = TrainDatasetFromFolder('/content/drive/MyDrive/datasets/DIV2K/DIV2K_train_HR', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n","val_set = ValDatasetFromFolder('/content/drive/MyDrive/datasets/DIV2K/DIV2K_train_HR', upscale_factor=UPSCALE_FACTOR)\n","train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n","val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n","\n","netG = Generator(UPSCALE_FACTOR)\n","print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n","netD = Discriminator()\n","print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n","\n","generator_criterion = GeneratorLoss()\n","\n","if torch.cuda.is_available():\n","    netG.cuda()\n","    netD.cuda()\n","    generator_criterion.cuda()\n","\n","optimizerG = optim.Adam(netG.parameters())\n","optimizerD = optim.Adam(netD.parameters())\n","\n","results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n","\n","for epoch in range(1, NUM_EPOCHS + 1):\n","    train_bar = tqdm(train_loader)\n","    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n","\n","    netG.train()\n","    netD.train()\n","    for data, target in train_bar:\n","        g_update_first = True\n","        batch_size = data.size(0)\n","        running_results['batch_sizes'] += batch_size\n","\n","        ############################\n","        # (1) Update D network: maximize D(x)-1-D(G(z))\n","        ###########################\n","        real_img = Variable(target)\n","        if torch.cuda.is_available():\n","            real_img = real_img.cuda()\n","        z = Variable(data)\n","        if torch.cuda.is_available():\n","                z = z.cuda()\n","        fake_img = netG(z)\n","\n","        netD.zero_grad()\n","        real_out = netD(real_img).mean()\n","        fake_out = netD(fake_img).mean()\n","        d_loss = 1 - real_out + fake_out\n","        d_loss.backward(retain_graph=True)\n","        optimizerD.step()\n","\n","        ############################\n","        # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n","        ###########################\n","        netG.zero_grad()\n","        ## The two lines below are added to prevent runetime error in Google Colab ##\n","        fake_img = netG(z)\n","        fake_out = netD(fake_img).mean()\n","        ##\n","        g_loss = generator_criterion(fake_out, fake_img, real_img)\n","        g_loss.backward()\n","\n","        fake_img = netG(z)\n","        fake_out = netD(fake_img).mean()\n","\n","\n","        optimizerG.step()\n","\n","        # loss for current batch before optimization\n","        running_results['g_loss'] += g_loss.item() * batch_size\n","        running_results['d_loss'] += d_loss.item() * batch_size\n","        running_results['d_score'] += real_out.item() * batch_size\n","        running_results['g_score'] += fake_out.item() * batch_size\n","\n","        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n","            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n","            running_results['g_loss'] / running_results['batch_sizes'],\n","            running_results['d_score'] / running_results['batch_sizes'],\n","            running_results['g_score'] / running_results['batch_sizes']))\n","\n","    netG.eval()\n","    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n","    if not os.path.exists(out_path):\n","        os.makedirs(out_path)\n","\n","    with torch.no_grad():\n","        val_bar = tqdm(val_loader)\n","        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n","        val_images = []\n","        for val_lr, val_hr_restore, val_hr in val_bar:\n","            batch_size = val_lr.size(0)\n","            valing_results['batch_sizes'] += batch_size\n","            lr = val_lr\n","            hr = val_hr\n","            if torch.cuda.is_available():\n","                lr = lr.cuda()\n","                hr = hr.cuda()\n","            sr = netG(lr)\n","\n","            batch_mse = ((sr - hr) ** 2).data.mean()\n","            valing_results['mse'] += batch_mse * batch_size\n","            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n","            valing_results['ssims'] += batch_ssim * batch_size\n","            valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n","            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n","            val_bar.set_description(\n","                desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n","                    valing_results['psnr'], valing_results['ssim']))\n","\n","            val_images.extend(\n","                [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n","                  display_transform()(sr.data.cpu().squeeze(0))])\n","        val_images = torch.stack(val_images)\n","        val_images = torch.chunk(val_images, val_images.size(0) // 15)\n","        val_save_bar = tqdm(val_images, desc='[saving training results]')\n","        index = 1\n","        for image in val_save_bar:\n","            image = utils.make_grid(image, nrow=3, padding=5)\n","            utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n","            index += 1\n","\n","    # save model parameters\n","    torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n","    torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n","    # save loss\\scores\\psnr\\ssim\n","    results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n","    results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n","    results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n","    results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n","    results['psnr'].append(valing_results['psnr'])\n","    results['ssim'].append(valing_results['ssim'])\n","\n","    if epoch % 10 == 0 and epoch != 0:\n","        out_path = 'statistics/'\n","        data_frame = pd.DataFrame(\n","            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n","                  'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n","            index=range(1, epoch + 1))\n","        data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"63pTZRSLrfgx","executionInfo":{"status":"error","timestamp":1712414480126,"user_tz":-480,"elapsed":1065467,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}},"outputId":"64d4dce9-dfd2-45aa-ca27-f9634aa6feae"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["# generator parameters: 734219\n","# discriminator parameters: 5215425\n"]},{"output_type":"stream","name":"stderr","text":["[1/1] Loss_D: 0.6958 Loss_G: 0.0387 D(x): 0.5474 D(G(z)): 0.1735: 100%|██████████| 15/15 [17:11<00:00, 68.78s/it]\n","  0%|          | 0/900 [00:31<?, ?it/s]\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'pytorch_ssim' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-e49ae40b9794>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mbatch_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mvaling_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_mse\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mbatch_ssim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_ssim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mvaling_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ssims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_ssim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mvaling_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'psnr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvaling_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvaling_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_sizes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pytorch_ssim' is not defined"]}]},{"cell_type":"code","source":["print(os.getcwd())\n","os.chdir('drive/MyDrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9EFoown6rzYY","executionInfo":{"status":"ok","timestamp":1712411647625,"user_tz":-480,"elapsed":440,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}},"outputId":"d8466349-70dd-49ec-a5e2-5b64dadd4408"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]}]}