{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1-bXdT6pEFsED5e2AaEcjaImzvMtYiEpJ","authorship_tag":"ABX9TyMwJJx7sZCsPtNwQGHG/73B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# EDSR\n","\n","Enhanced Deep Residual Networks for Single Image Super-Resolution， CVPR 2017"],"metadata":{"id":"2KD-trbSE3lj"}},{"cell_type":"markdown","source":["## 准备数据集"],"metadata":{"id":"wT2jjDpmE_A7"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"UeITpHuvE27A","executionInfo":{"status":"ok","timestamp":1712407532976,"user_tz":-480,"elapsed":1202895,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"outputs":[],"source":["!wget -q https://cv.snu.ac.kr/research/EDSR/DIV2K.tar\n","!tar -xf DIV2K.tar"]},{"cell_type":"markdown","source":["## 引入依赖包"],"metadata":{"id":"ZTipGF7kMLS1"}},{"cell_type":"code","source":["import torch\n","import torch.utils.data as data\n","\n","import os\n","import glob\n","import pickle\n","import random\n","import imageio\n","import math\n","from PIL import Image"],"metadata":{"id":"j-bCIhqZMPHE","executionInfo":{"status":"ok","timestamp":1712408461925,"user_tz":-480,"elapsed":472,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["print(os.getcwd())\n","os.chdir('EDSR-PyTorch')\n","print(os.getcwd())\n","os.chdir('..')\n","print(os.getcwd())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bikIfXfjf4nP","executionInfo":{"status":"ok","timestamp":1712408572694,"user_tz":-480,"elapsed":435,"user":{"displayName":"yitong wang","userId":"06814284011318390648"}},"outputId":"6fb3a4c1-93b2-45a1-cc25-5460709166a2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content/EDSR-PyTorch\n","/content\n"]}]},{"cell_type":"markdown","source":["## 数据集类"],"metadata":{"id":"UKRCPAATavEj"}},{"cell_type":"code","source":["class SRData(data.Dataset):\n","    def __init__(self, args, name='', train=True, benchmark=False):\n","        self.args = args\n","        self.name = name\n","        self.train = train\n","        self.split = 'train' if train else 'test'\n","        self.do_eval = True\n","        self.benchmark = benchmark\n","        self.input_large = (args.model == 'VDSR')\n","        self.scale = args.scale\n","        self.idx_scale = 0\n","\n","        self._set_filesystem(args.dir_data)\n","        if args.ext.find('img') < 0:\n","            path_bin = os.path.join(self.apath, 'bin')\n","            os.makedirs(path_bin, exist_ok=True)\n","\n","        list_hr, list_lr = self._scan()\n","        if args.ext.find('img') >= 0 or benchmark:\n","            self.images_hr, self.images_lr = list_hr, list_lr\n","        elif args.ext.find('sep') >= 0:\n","            os.makedirs(\n","                self.dir_hr.replace(self.apath, path_bin),\n","                exist_ok=True\n","            )\n","            for s in self.scale:\n","                os.makedirs(\n","                    os.path.join(\n","                        self.dir_lr.replace(self.apath, path_bin),\n","                        'X{}'.format(s)\n","                    ),\n","                    exist_ok=True\n","                )\n","\n","            self.images_hr, self.images_lr = [], [[] for _ in self.scale]\n","            for h in list_hr:\n","                b = h.replace(self.apath, path_bin)\n","                b = b.replace(self.ext[0], '.pt')\n","                self.images_hr.append(b)\n","                self._check_and_load(args.ext, h, b, verbose=True)\n","            for i, ll in enumerate(list_lr):\n","                for l in ll:\n","                    b = l.replace(self.apath, path_bin)\n","                    b = b.replace(self.ext[1], '.pt')\n","                    self.images_lr[i].append(b)\n","                    self._check_and_load(args.ext, l, b, verbose=True)\n","        if train:\n","            n_patches = args.batch_size * args.test_every\n","            n_images = len(args.data_train) * len(self.images_hr)\n","            if n_images == 0:\n","                self.repeat = 0\n","            else:\n","                self.repeat = max(n_patches // n_images, 1)\n","\n","    # Below functions as used to prepare images\n","    def _scan(self):\n","        names_hr = sorted(\n","            glob.glob(os.path.join(self.dir_hr, '*' + self.ext[0]))\n","        )\n","        names_lr = [[] for _ in self.scale]\n","        for f in names_hr:\n","            filename, _ = os.path.splitext(os.path.basename(f))\n","            for si, s in enumerate(self.scale):\n","                names_lr[si].append(os.path.join(\n","                    self.dir_lr, 'X{}/{}x{}{}'.format(\n","                        s, filename, s, self.ext[1]\n","                    )\n","                ))\n","\n","        return names_hr, names_lr\n","\n","    def _set_filesystem(self, dir_data):\n","        self.apath = os.path.join(dir_data, self.name)\n","        self.dir_hr = os.path.join(self.apath, 'HR')\n","        self.dir_lr = os.path.join(self.apath, 'LR_bicubic')\n","        if self.input_large: self.dir_lr += 'L'\n","        self.ext = ('.png', '.png')\n","\n","    def _check_and_load(self, ext, img, f, verbose=True):\n","        if not os.path.isfile(f) or ext.find('reset') >= 0:\n","            if verbose:\n","                print('Making a binary: {}'.format(f))\n","            with open(f, 'wb') as _f:\n","                pickle.dump(imageio.imread(img), _f)\n","\n","    def __getitem__(self, idx):\n","        lr, hr, filename = self._load_file(idx)\n","        pair = self.get_patch(lr, hr)\n","        pair = common.set_channel(*pair, n_channels=self.args.n_colors)\n","        pair_t = common.np2Tensor(*pair, rgb_range=self.args.rgb_range)\n","\n","        return pair_t[0], pair_t[1], filename\n","\n","    def __len__(self):\n","        if self.train:\n","            return len(self.images_hr) * self.repeat\n","        else:\n","            return len(self.images_hr)\n","\n","    def _get_index(self, idx):\n","        if self.train:\n","            return idx % len(self.images_hr)\n","        else:\n","            return idx\n","\n","    def _load_file(self, idx):\n","        idx = self._get_index(idx)\n","        f_hr = self.images_hr[idx]\n","        f_lr = self.images_lr[self.idx_scale][idx]\n","\n","        filename, _ = os.path.splitext(os.path.basename(f_hr))\n","        if self.args.ext == 'img' or self.benchmark:\n","            hr = imageio.imread(f_hr)\n","            lr = imageio.imread(f_lr)\n","        elif self.args.ext.find('sep') >= 0:\n","            with open(f_hr, 'rb') as _f:\n","                hr = pickle.load(_f)\n","            with open(f_lr, 'rb') as _f:\n","                lr = pickle.load(_f)\n","\n","        return lr, hr, filename\n","\n","    def get_patch(self, lr, hr):\n","        scale = self.scale[self.idx_scale]\n","        if self.train:\n","            lr, hr = common.get_patch(\n","                lr, hr,\n","                patch_size=self.args.patch_size,\n","                scale=scale,\n","                multi=(len(self.scale) > 1),\n","                input_large=self.input_large\n","            )\n","            if not self.args.no_augment: lr, hr = common.augment(lr, hr)\n","        else:\n","            ih, iw = lr.shape[:2]\n","            hr = hr[0:ih * scale, 0:iw * scale]\n","\n","        return lr, hr\n","\n","    def set_scale(self, idx_scale):\n","        if not self.input_large:\n","            self.idx_scale = idx_scale\n","        else:\n","            self.idx_scale = random.randint(0, len(self.scale) - 1)"],"metadata":{"id":"n2ZaYzTzaybj"},"execution_count":null,"outputs":[]}]}